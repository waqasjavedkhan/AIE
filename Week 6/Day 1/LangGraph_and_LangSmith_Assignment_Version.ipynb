{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJXW_DgiSebM"
      },
      "source": [
        "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n",
        "\n",
        "In the following notebook we'll complete the following tasks:\n",
        "\n",
        "- ü§ù Breakout Room #1:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Creating our Tool Belt\n",
        "  4. Creating Our State\n",
        "  5. Creating and Compiling A Graph!\n",
        "  \n",
        "\n",
        "- ü§ù Breakout Room #2:\n",
        "  1. Creating an Evaluation Dataset\n",
        "  2. Adding Evaluators\n",
        "  3. Evaluating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQ3nRAgoF67"
      },
      "source": [
        "# ü§ù Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7pQDUhUnIo8"
      },
      "source": [
        "## LangGraph - Building Cyclic Applications with LangChain\n",
        "\n",
        "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
        "\n",
        "### Why Cycles?\n",
        "\n",
        "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
        "\n",
        "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effetively allowing us to recreate appliation flowcharts in code in an almost 1-to-1 fashion.\n",
        "\n",
        "### Why LangGraph?\n",
        "\n",
        "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_fLDElOVoop"
      },
      "source": [
        "## Task 1:  Dependencies\n",
        "\n",
        "We'll first install all our required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaVwN269EttM",
        "outputId": "5dc553cf-9b59-400f-9a18-a73223ffce2b"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain_openai langgraph arxiv duckduckgo-search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujPjGJuoPwg"
      },
      "source": [
        "## Task 2: Environment Variables\n",
        "\n",
        "We'll want to set both our OpenAI API key and our LangSmith environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdh8CoVWHRvs",
        "outputId": "b2b4f266-fec0-447e-c828-cd01887b9919"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0glIDyHmRt",
        "outputId": "7eb41b6b-9a3f-4a89-df6d-437e2f504f7b"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE1 - LangGraph - {uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRyQmEAVzua"
      },
      "source": [
        "## Task 3: Creating our Tool Belt\n",
        "\n",
        "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
        "\n",
        "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
        "\n",
        "We'll leverage:\n",
        "\n",
        "- [Duck Duck Go Web Search](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/ddg_search)\n",
        "- [Arxiv](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k6n_Dob2F46"
      },
      "source": [
        "####üèóÔ∏è Activity #1:\n",
        "\n",
        "Please add the tools to use into our toolbelt.\n",
        "\n",
        "> NOTE: Each tool in our toolbelt should be a method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lAxaSvlfIeOg"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "\n",
        "tool_belt = [\n",
        "    DuckDuckGoSearchRun(description='A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current topics. Input should be a search query.'),\n",
        "    ArxivQueryRun(description='A wrapper around Arxiv Search. Useful for when you need to answer questions of scientific or technical nature. Input should be a search query.'),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FdOjEslXdRR"
      },
      "source": [
        "### Actioning with Tools\n",
        "\n",
        "Now that we've created our tool belt - we need to create a process that will let us leverage them when we need them.\n",
        "\n",
        "We'll use the built-in [`ToolExecutor`](https://github.com/langchain-ai/langgraph/blob/main/langgraph/prebuilt/tool_executor.py) to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "cFr1m80-JZsD"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolExecutor\n",
        "\n",
        "tool_executor = ToolExecutor(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI-C669ZYVI5"
      },
      "source": [
        "### Model\n",
        "\n",
        "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
        "\n",
        "- OpenAI's GPT-3.5 and GPT-4\n",
        "- Anthropic's Claude\n",
        "- Google's Gemini\n",
        "\n",
        "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugkj3GzuZpQv"
      },
      "source": [
        "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "4OdMqFafZ_0V"
      },
      "outputs": [],
      "source": [
        "from langchain_core.utils.function_calling import convert_to_openai_function\n",
        "\n",
        "functions = [convert_to_openai_function(t) for t in tool_belt]\n",
        "model = model.bind_functions(functions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERzuGo6W18Lr"
      },
      "source": [
        "#### ‚ùì Question #1:\n",
        "\n",
        "How does the model determine which tool to use?\n",
        "\n",
        "The model determines which tool to use based on the input it receives and the predefined workflow or logic encoded within the LangChain application. When a question or task is presented to the model, it assesses the requirements of the query against the capabilities of the tools available in its \"tool belt.\" The decision-making process involves analyzing the input's context, the nature of the task at hand, and any state information stored from previous interactions.\n",
        "\n",
        "In LangChain, tools are typically associated with specific types of tasks, such as searching for information, processing language, or generating responses. The model uses a combination of natural language understanding techniques and predefined logic to match the input task with the most suitable tool. This might involve keyword detection, pattern matching, or more sophisticated NLP techniques to understand the intent and requirements of the input.\n",
        "\n",
        "Moreover, if the application utilizes LangGraph to create a cyclic application, the graph's structure and the conditional edges defined within it can also play a critical role in determining the path of execution and thereby which tool gets used. Conditional edges can route the process flow based on the current context or the output of previous nodes, allowing the model to dynamically select the appropriate tool for each situation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_296Ub96Z_H8"
      },
      "source": [
        "## Putting the State in Stateful\n",
        "\n",
        "Earlier we used this phrasing:\n",
        "\n",
        "`coordinated multi-actor and stateful applications`\n",
        "\n",
        "So what does that \"stateful\" mean?\n",
        "\n",
        "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
        "\n",
        "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
        "\n",
        "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
        "\n",
        "1. We initialize our state object:\n",
        "  - `{\"messages\" : []}`\n",
        "2. Our user submits a query to our application.\n",
        "  - New State: `HumanMessage(#1)`\n",
        "  - `{\"messages\" : [HumanMessage(#1)}`\n",
        "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
        "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
        "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
        "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated, Sequence\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[Sequence[BaseMessage], operator.add]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWsMhfO9grLu"
      },
      "source": [
        "## It's Graphing Time!\n",
        "\n",
        "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
        "\n",
        "Let's take a second to refresh ourselves about what a graph is in this context.\n",
        "\n",
        "Graphs, also called networks in some circles, are a collection of connected objects.\n",
        "\n",
        "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
        "\n",
        "Let's look at a simple graph.\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n",
        "\n",
        "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
        "\n",
        "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
        "\n",
        "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
        "\n",
        "Let's create some nodes and expand on our diagram.\n",
        "\n",
        "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "91flJWtZLUrl"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolInvocation\n",
        "import json\n",
        "from langchain_core.messages import FunctionMessage\n",
        "\n",
        "def call_model(state):\n",
        "  messages = state[\"messages\"]\n",
        "  response = model.invoke(messages)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "def call_tool(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  action = ToolInvocation(\n",
        "      tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
        "      tool_input=json.loads(\n",
        "          last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
        "      )\n",
        "  )\n",
        "\n",
        "  response = tool_executor.invoke(action)\n",
        "\n",
        "  function_message = FunctionMessage(content=str(response), name=action.tool)\n",
        "\n",
        "  return {\"messages\" : [function_message]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bwR7MgWj3Wg"
      },
      "source": [
        "Now we have two total nodes. We have:\n",
        "\n",
        "- `call_model` is a node that will...well...call the model\n",
        "- `call_tool` is a node which will call a tool\n",
        "\n",
        "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "_vF4_lgtmQNo"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"action\", call_tool)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8CjRlbVmRpW"
      },
      "source": [
        "Let's look at what we have so far:\n",
        "\n",
        "![image](https://i.imgur.com/md7inqG.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaXHpPeSnOWC"
      },
      "source": [
        "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "YGCbaYqRnmiw"
      },
      "outputs": [],
      "source": [
        "workflow.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUsfGoSpoF9U"
      },
      "source": [
        "![image](https://i.imgur.com/wNixpJe.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q_pQgHmoW0M"
      },
      "source": [
        "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
        "\n",
        "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
        "\n",
        "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
        "\n",
        "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
        "\n",
        "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "1BZgb81VQf9o"
      },
      "outputs": [],
      "source": [
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if \"function_call\" not in last_message.additional_kwargs:\n",
        "    return \"end\"\n",
        "\n",
        "  return \"continue\"\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cvhcf4jp0Ce"
      },
      "source": [
        "Let's visualize what this looks like.\n",
        "\n",
        "![image](https://i.imgur.com/8ZNwKI5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKCjWJCkrJb9"
      },
      "source": [
        "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "UvcgbHf1rIXZ"
      },
      "outputs": [],
      "source": [
        "workflow.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiWDwBQtrw7Z"
      },
      "source": [
        "Let's look at the final visualization.\n",
        "\n",
        "![image](https://i.imgur.com/NWO7usO.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYqDpErlsCsu"
      },
      "source": [
        "All that's left to do now is to compile our workflow - and we're off!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "zt9-KS8DpzNx"
      },
      "outputs": [],
      "source": [
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhNWIwBL1W4Q"
      },
      "source": [
        "#### ‚ùì Question #2:\n",
        "\n",
        "- Is there any specific limit to how many times we can cycle?\n",
        "\n",
        "By default, LangGraph does not impose a strict limit on the number of cycles a workflow can execute. The cycle count is primarily constrained by practical considerations such as execution time, resource availability, and the specific requirements of the task at hand. In an open-ended cycle, the workflow might continue until it meets a certain condition or until external factors (such as a timeout or resource limitation) halt the process.\n",
        "\n",
        "- How could we impose a limit to the number of cycles?\n",
        "\n",
        "To impose a limit on the number of cycles, we can introduce a stateful mechanism that tracks the number of iterations a workflow has completed. Here are a few approaches to achieve this:\n",
        "\n",
        "- State Variable: Incorporate a state variable into the workflow that counts the number of cycles. At each iteration, the variable is incremented. Conditional logic can then be used to check if the count has reached a predefined maximum, at which point the cycle can be terminated or redirected to a concluding node.\n",
        "\n",
        "- Conditional Edges: Use conditional edges within LangGraph to evaluate the current cycle count against a threshold. If the count exceeds the limit, the workflow can be directed towards an exit path or a node designed to handle such situations.\n",
        "\n",
        "- Limit in the Application Logic: Implement the cycle limit directly in the application logic that orchestrates the workflow. This approach allows for more flexibility in managing how the limit is enforced and can incorporate additional logic based on the outcome of reaching the limit.\n",
        "\n",
        "- Time-based Limit: Instead of, or in addition to, a count-based limit, a time-based limit could be set to ensure that the workflow does not exceed a certain duration. This approach might be useful in scenarios where execution time is a critical factor.\n",
        "\n",
        "- Evaluation Control: We can add an Evaluation Control which can determine if we met the required Evaluation Criteria and we can halt the workflow. \n",
        "\n",
        "By implementing such mechanisms, we can control the execution flow of cyclic applications within LangGraph, ensuring that workflows complete in a timely manner and do not consume excessive resources. ‚Äã‚Äã"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEYcTShCsPaa"
      },
      "source": [
        "## Using Our Graph\n",
        "\n",
        "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
        "\n",
        "Let's try out a few examples to see how it fairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn4n37PQRPII",
        "outputId": "227d84f3-a86e-4a18-b01d-7c60def5a260"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='What is RAG in the context of Large Language Models? When did it break onto the scene?'),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"RAG in the context of Large Language Models\"}', 'name': 'duckduckgo_search'}}, response_metadata={'finish_reason': 'function_call', 'logprobs': None}),\n",
              "  FunctionMessage(content='Key Takeaways. RAG is a relatively new artificial intelligence technique that can improve the quality of generative AI by allowing large language model (LLMs) to tap additional data resources without retraining. RAG models build knowledge repositories based on the organization\\'s own data, and the repositories can be continually updated to ... The beauty of RAG lies in its ability to enable a language model to draw upon and leverage your own data to generate responses. While base models are traditionally trained on specific, point-in-time data, ensuring their effectiveness in performing tasks and adapting to the desired domain, they can struggle when faced with newer or current data. Retrieval-augmented generation (RAG) is a technique used to \"ground\" large language models (LLMs) with specific data sources, often sources that weren\\'t included in the models\\' original ... RAG stands for R etrieval- A ugmented G eneration. RAG enables large language models (LLM) to access and utilize up-to-date information. Hence, it improves the quality of relevance of the response from LLM. Below is a simple diagram of how RAG is implemented. The rise of Large Language Models (LLMs) like GPT4 or BARD has been nothing short of revolutionary, offering unparalleled capabilities in natural language understanding and generation. However, these sophisticated models are not without their limitations. ... RAG systems provide context-aware recommendations that align with user needs and ...', name='duckduckgo_search'),\n",
              "  AIMessage(content=\"RAG stands for Retrieval-Augmented Generation in the context of Large Language Models (LLMs). It is a relatively new artificial intelligence technique that improves the quality of generative AI by allowing LLMs to tap into additional data resources without the need for retraining. RAG models build knowledge repositories based on the organization's own data, which can be continually updated to enhance the model's performance.\\n\\nRAG enables LLMs to access and utilize up-to-date information, improving the relevance and quality of the responses generated by the model. It allows language models to draw upon and leverage specific data sources, even those that were not included in the model's original training data.\\n\\nRAG broke onto the scene with the rise of Large Language Models like GPT-4 or BERT, offering unparalleled capabilities in natural language understanding and generation. The technique of Retrieval-Augmented Generation has been instrumental in addressing the limitations of these sophisticated models and providing context-aware recommendations that align with user needs.\", response_metadata={'finish_reason': 'stop', 'logprobs': None})]}"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"What is RAG in the context of Large Language Models? When did it break onto the scene?\")]}\n",
        "\n",
        "app.invoke(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBHnUtLSscRr"
      },
      "source": [
        "Let's look at what happened:\n",
        "\n",
        "1. Our state object was populated with our request\n",
        "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "3. The conditional edge received the state object, found the \"function_call\" `additional_kwarg`, and sent the state object to the action node\n",
        "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
        "5. The agent node added a response to the state object and passed it along the conditional edge\n",
        "6. The conditional edge received the state object, could not find the \"function_call\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
        "\n",
        "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afv2BuEsV5JG",
        "outputId": "a1cda958-d781-415c-deda-c2c92097e99f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='What is QLoRA in Machine Learning? Are their any papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?'),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"QLoRA in Machine Learning\"}', 'name': 'duckduckgo_search'}}, response_metadata={'finish_reason': 'function_call', 'logprobs': None}),\n",
              "  FunctionMessage(content=\"Balancing this tradeoff is a key contribution of QLoRA. QLoRA. QLoRA (or Quantized Low-Rank Adaptation) combines 4 ingredients to get the most out of a machine's limited memory without sacrificing model performance. I will briefly summarize key points from each. More details are available in the QLoRA paper [4]. Ingredient 1: 4-bit NormalFloat As the world of machine learning evolves, tools like HuggingFace and the open-source LLMs movement have significantly simplified the‚Ä¶ ¬∑ 12 min read ¬∑ Jan 18, 2024 1 Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. QLoRA is a fine-tuning method that combines Quantization and Low-Rank Adapters (LoRA). QLoRA is revolutionary in that it democratizes fine-tuning: it enables one to fine-tune massive models with billions of parameters on relatively small, highly available GPUs. ... QLoRA stands as a significant advancement in the field of machine learning ... Large Language Models (LLMs) are currently a hot topic in the field of machine learning. Imagine you're an ML Engineer and your company has access to GPUs and open-source LLMs like LLAMA/Falcon. ... LoRA, Machine Learning, QLoRA, transformers. Categories: data-science, machine-learning. Updated: July 26, 2023. Share on Twitter Facebook ...\", name='duckduckgo_search'),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"QLoRA in Machine Learning\"}', 'name': 'arxiv'}}, response_metadata={'finish_reason': 'function_call', 'logprobs': None}),\n",
              "  FunctionMessage(content=\"Published: 2023-05-23\\nTitle: QLoRA: Efficient Finetuning of Quantized LLMs\\nAuthors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\\nSummary: We present QLoRA, an efficient finetuning approach that reduces memory usage\\nenough to finetune a 65B parameter model on a single 48GB GPU while preserving\\nfull 16-bit finetuning task performance. QLoRA backpropagates gradients through\\na frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\\nprevious openly released models on the Vicuna benchmark, reaching 99.3% of the\\nperformance level of ChatGPT while only requiring 24 hours of finetuning on a\\nsingle GPU. QLoRA introduces a number of innovations to save memory without\\nsacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\\ninformation theoretically optimal for normally distributed weights (b) double\\nquantization to reduce the average memory footprint by quantizing the\\nquantization constants, and (c) paged optimziers to manage memory spikes. We\\nuse QLoRA to finetune more than 1,000 models, providing a detailed analysis of\\ninstruction following and chatbot performance across 8 instruction datasets,\\nmultiple model types (LLaMA, T5), and model scales that would be infeasible to\\nrun with regular finetuning (e.g. 33B and 65B parameter models). Our results\\nshow that QLoRA finetuning on a small high-quality dataset leads to\\nstate-of-the-art results, even when using smaller models than the previous\\nSoTA. We provide a detailed analysis of chatbot performance based on both human\\nand GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\\nalternative to human evaluation. Furthermore, we find that current chatbot\\nbenchmarks are not trustworthy to accurately evaluate the performance levels of\\nchatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\\nChatGPT. We release all of our models and code, including CUDA kernels for\\n4-bit training.\\n\\nPublished: 2023-12-31\\nTitle: Viz: A QLoRA-based Copyright Marketplace for Legally Compliant Generative AI\\nAuthors: Dipankar Sarkar\\nSummary: This paper aims to introduce and analyze the Viz system in a comprehensive\\nway, a novel system architecture that integrates Quantized Low-Rank Adapters\\n(QLoRA) to fine-tune large language models (LLM) within a legally compliant and\\nresource efficient marketplace. Viz represents a significant contribution to\\nthe field of artificial intelligence, particularly in addressing the challenges\\nof computational efficiency, legal compliance, and economic sustainability in\\nthe utilization and monetization of LLMs. The paper delineates the scholarly\\ndiscourse and developments that have informed the creation of Viz, focusing\\nprimarily on the advancements in LLM models, copyright issues in AI training\\n(NYT case, 2023), and the evolution of model fine-tuning techniques,\\nparticularly low-rank adapters and quantized low-rank adapters, to create a\\nsustainable and economically compliant framework for LLM utilization. The\\neconomic model it proposes benefits content creators, AI developers, and\\nend-users, delineating a harmonious integration of technology, economy, and\\nlaw, offering a comprehensive solution to the complex challenges of today's AI\\nlandscape.\\n\\nPublished: 2024-02-08\\nTitle: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nAuthors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\\nSummary: The LoRA-finetuning quantization of LLMs has been extensively studied to\\nobtain accurate yet compact LLMs for deployment on resource-constrained\\nhardware. However, existing methods cause the quantized LLM to severely degrade\\nand even fail to benefit from the finetuning of LoRA. This paper proposes a\\nnovel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\\nthrough information retention. The proposed IR-QLoRA mainly relies on two\\ntechnolog\", name='arxiv'),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"Tim Dettmers bio\"}', 'name': 'duckduckgo_search'}}, response_metadata={'finish_reason': 'function_call', 'logprobs': None}),\n",
              "  FunctionMessage(content=\"Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Tim Dettmers on quantization of neural networks ‚Äî #41 - YouTube. Lots of fascinating insights in an interview between Steve Hsu and Tim Dettmers, quantization savant extraordinaire. Very techy. Below is the Claude 2 summary I have corrected, accurate based on my listening but doesn't convey how deep in the reeds Dettmers gets. ... Sponsored by Evolution AI: https://www.evolution.aiAbstract: Recent open-source large language models (LLMs) like LLaMA and Falcon are both high-quality and ... Episode 82 of the Stanford MLSys Seminar Series!Democratizing Foundation Models via k-bit QuantizationSpeaker: Tim DettmersAbstract:Foundation models are eff...\", name='duckduckgo_search'),\n",
              "  AIMessage(content='QLoRA (Quantized Low-Rank Adaptation) is an efficient fine-tuning approach in machine learning that reduces memory usage while preserving full performance. It combines quantization and Low Rank Adapters (LoRA) to enable fine-tuning of massive models with billions of parameters on relatively small GPUs. The QLoRA paper titled \"QLoRA: Efficient Finetuning of Quantized LLMs\" was authored by Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. The paper introduces innovations like 4-bit NormalFloat (NF4) data type and double quantization to save memory without sacrificing performance.\\n\\nTim Dettmers, one of the authors of the QLoRA paper, focuses on making foundation models like ChatGPT accessible by reducing their resource requirements. He works on developing compression and networking algorithms to enable memory-efficient, fast, and cost-effective deep learning solutions.', response_metadata={'finish_reason': 'stop', 'logprobs': None})]}"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"What is QLoRA in Machine Learning? Are their any papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?\")]}\n",
        "\n",
        "app.invoke(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXzDlZVz1Hnf"
      },
      "source": [
        "####üèóÔ∏è Activity #2:\n",
        "\n",
        "Please write out the steps the agent took to arrive at the correct answer.\n",
        "\n",
        "1. Receiving the Input: The agent started by receiving an input question. This input is the starting point for the workflow.\n",
        "\n",
        "2. Parsing the Input: The input is analyzed to understand its nature and requirements. This involves natural language processing techniques to extract key information, intent, and context.\n",
        "\n",
        "3. Selecting the Starting Node: Based on the input analysis, the agent selected the appropriate starting node in the LangGraph workflow. \n",
        "\n",
        "4. Executing Graph Nodes: The agent proceeded through the graph by executing nodes in sequence. Each node represents a specific action or processing step.\n",
        "\n",
        "5. Data Retrieval: The agent used a node designed to fetch data from external sources.\n",
        "\n",
        "6. Processing and Analysis: Data and input were processed and analyzed, involving additional tools.\n",
        "\n",
        "7. Following Conditional Paths: The workflow included conditional edges that dictated the flow based on current data or state. The agent evaluated these conditions to decide the next steps.\n",
        "\n",
        "8. Iterating if Necessary: In this cyclic workflow, the agent might loop through certain steps multiple times, refining its understanding or gathering more information as needed.\n",
        "\n",
        "9. Formulating the Answer: Once all relevant information had been processed and the necessary computations were complete, the agent formulated an answer or response.\n",
        "\n",
        "10. Outputting the Answer: The final step involved presenting the answer to the user.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV3XeFOT1Sar"
      },
      "source": [
        "### Pre-processing for LangSmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wruQCuzewUuO"
      },
      "source": [
        "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "oeXdQgbxwhTv"
      },
      "outputs": [],
      "source": [
        "def convert_inputs(input_object):\n",
        "  return {\"messages\" : [HumanMessage(content=input_object[\"question\"])]}\n",
        "\n",
        "def parse_output(input_state):\n",
        "  return input_state[\"messages\"][-1].content\n",
        "\n",
        "agent_chain = convert_inputs | app | parse_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "orYxBZXSxJjZ",
        "outputId": "3894f1b2-4259-4ba9-fe41-69bd48f80406"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"RAG stands for Retrieval-augmented generation (RAG). It is an AI framework that enhances generative AI models with facts from external sources to improve the quality of generated responses. RAG combines retrieval-based and generative models to enhance AI systems' understanding and generation of human-like text.\""
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_chain.invoke({\"question\" : \"What is RAG?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQmrzYfrm1Dr"
      },
      "source": [
        "# ü§ù Breakout Room #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9UkCIqkpyZu"
      },
      "source": [
        "## Task 1: Creating An Evaluation Dataset\n",
        "\n",
        "Just as we saw last week, we'll want to create a dataset to test our Agent's ability to answer questions.\n",
        "\n",
        "In order to do this - we'll want to provide some questions and some answers. Let's look at how we can create such a dataset below.\n",
        "\n",
        "```python\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfMXF2KAsQxs"
      },
      "source": [
        "####üèóÔ∏è Activity #3:\n",
        "\n",
        "Please create a dataset in the above format with at least 5 questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "CbagRuJop83E"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"What is the capital of France?\",\n",
        "    \"Who is the author of 'To Kill a Mockingbird'?\",\n",
        "    \"What is the chemical symbol for gold?\",\n",
        "    \"What is the largest planet in our solar system?\",\n",
        "    \"Who painted the Mona Lisa?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\": [\"Paris\"]},\n",
        "    {\"must_mention\": [\"Harper Lee\"]},\n",
        "    {\"must_mention\": [\"Au\"]},\n",
        "    {\"must_mention\": [\"Jupiter\"]},\n",
        "    {\"must_mention\": [\"Leonardo da Vinci\"]}\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7QVFuAmsh7L"
      },
      "source": [
        "Now we can add our dataset to our LangSmith project using the following code which we saw last Thursday!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "RLfrZrgSsn85"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "dataset_name = f\"Retrieval Augmented Generation - Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    outputs=answers,\n",
        "    dataset_id=dataset.id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciV73F9Q04w0"
      },
      "source": [
        "#### ‚ùì Question #3:\n",
        "\n",
        "How are the correct answers associated with the questions?\n",
        "\n",
        "> NOTE: Feel free to indicate if this is problematic or not\n",
        "\n",
        "This assignment involves tasks related to natural language processing, model evaluation, and the creation of an evaluation dataset for LangSmith, associating correct answers with questions is a crucial aspect. Here's how this process typically works and considerations regarding its potential problems:\n",
        "\n",
        "- Association Process\n",
        "\n",
        "Manual Annotation: In many cases, the correct answers are associated with questions through a process of manual annotation. Subject matter experts or annotators review the questions and provide the correct answers based on their knowledge or research. This method ensures high-quality and accurate associations but is time-consuming and labor-intensive.\n",
        "\n",
        "Automated Techniques: Automated methods might be used to associate answers with questions, especially in large datasets. This can involve using existing models to predict answers or extracting answers from source texts based on question context. While faster, these methods can introduce errors or biases based on the underlying models or data sources.\n",
        "\n",
        "Crowdsourcing: Crowdsourcing platforms allow for the distribution of annotation tasks to a large number of people, potentially speeding up the process and diversifying the pool of knowledge. However, this approach requires careful quality control to ensure the reliability of the associations.\n",
        "\n",
        "- Potential Problems\n",
        "\n",
        "Accuracy and Reliability: Ensuring the correctness of the associated answers is a significant challenge. Incorrect associations can lead to misleading evaluations of models trained or tested on the dataset.\n",
        "\n",
        "Bias: The process of associating answers with questions can introduce bias, particularly if the data sources or annotators have specific perspectives or if automated methods rely on biased datasets.\n",
        "\n",
        "Scalability: Manually associating answers with questions is not easily scalable to very large datasets commonly used in training and evaluating AI models.\n",
        "\n",
        "Ambiguity: Some questions might have multiple correct answers, or their answers might depend on context not captured within the dataset. Handling such ambiguities effectively is crucial for the utility of the dataset.\n",
        "\n",
        "In our context, associating correct answers with questions for evaluation purposes is likely managed through one or a combination of these methods. Addressing potential problems involves implementing robust quality control measures, considering the diversity of data and annotators, and possibly employing hybrid approaches to balance accuracy, bias, and scalability. ‚Äã‚Äã"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lRTXUrTtP9Y"
      },
      "source": [
        "## Task 2: Adding Evaluators\n",
        "\n",
        "Now we can add a custom evaluator to see if our responses contain the expected information.\n",
        "\n",
        "We'll be using a fairly naive exact-match process to determine if our response contains specific strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "QrAUXMFftlAY"
      },
      "outputs": [],
      "source": [
        "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
        "\n",
        "@run_evaluator\n",
        "def must_mention(run, example) -> EvaluationResult:\n",
        "    prediction = run.outputs.get(\"output\") or \"\"\n",
        "    required = example.outputs.get(\"must_mention\") or []\n",
        "    score = all(phrase in prediction for phrase in required)\n",
        "    return EvaluationResult(key=\"must_mention\", score=score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNtHORUh0jZY"
      },
      "source": [
        "#### ‚ùì Question #4:\n",
        "\n",
        "What are some ways you could improve this metric as-is?\n",
        "\n",
        "> NOTE: Alternatively you can suggest where gaps exist in this method.\n",
        "\n",
        "We can improve metrics by enhancing its ability to accurately reflect the performance, fairness, or reliability of the system it is measuring. Some of the strategies for improvement:\n",
        "\n",
        "- Enhancing Accuracy and Reliability\n",
        "\n",
        "Increase Data Diversity: Ensure the evaluation dataset covers a wide range of scenarios, languages, dialects, and domains. A diverse dataset helps in assessing the model's performance across different conditions.\n",
        "\n",
        "Incorporate Domain Expertise: Engaging domain experts in the development or refinement of metrics can provide insights that improve the metric's relevance and accuracy for specific applications.\n",
        "\n",
        "Refine Ground Truth: Improve the quality of the ground truth data used for evaluation. This could involve more rigorous annotation processes or revising existing annotations for clarity and correctness.\n",
        "\n",
        "- Addressing Bias\n",
        "\n",
        "Bias Detection and Correction: Implement methods to detect and correct bias in both the evaluation data and the metric itself. This might involve analyzing the metric's performance across various demographic groups or scenarios.\n",
        "\n",
        "Expand Evaluation Criteria: Introduce additional criteria or sub-metrics that specifically measure bias or fairness, providing a more holistic view of the model's performance.\n",
        "\n",
        "- Improving Interpretability\n",
        "\n",
        "Enhance Transparency: Make the metric more understandable by clearly explaining its components and how it calculates scores. Transparent metrics are more easily scrutinized and refined.\n",
        "\n",
        "User-Centric Design: Consider the end-users of the metric (e.g., researchers, developers, or policymakers) and adapt the metric to be more aligned with their needs and understanding.\n",
        "\n",
        "- Identifying Gaps\n",
        "\n",
        "Conduct Gap Analysis: Regularly review the metric's performance and its alignment with the desired outcomes. Identify areas where the metric fails to capture important aspects of performance or where it might produce misleading results.\n",
        "\n",
        "Engage with the Community: Solicit feedback from a broad range of stakeholders, including users affected by the model's decisions. Community input can highlight overlooked aspects or potential improvements.\n",
        "\n",
        "- Alternative Suggestions\n",
        "\n",
        "Multi-Metric Evaluation: Instead of relying on a single metric, use a combination of metrics that together provide a comprehensive evaluation of the model's performance.\n",
        "\n",
        "Dynamic Metrics: Develop metrics that can adapt over time to reflect changing societal norms, technology advancements, or shifts in the model's application context.\n",
        "Improving a metric or addressing gaps in its method involves a continuous process of assessment, feedback, and refinement. By applying these strategies, the effectiveness and fairness of AI systems can be better evaluated, leading to more reliable and equitable outcomes. ‚Äã‚Äã"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ4DVSXl0BX5"
      },
      "source": [
        "Now that we have created our custom evaluator - let's initialize our `RunEvalConfig` with it, and a few others:\n",
        "\n",
        "- `\"criteria\"` includes the default criteria which, in this case, means \"helpfulness\"\n",
        "- `\"cot_qa\"` includes a criteria that bases whether or not the answer is correct by utilizing a Chain of Thought prompt and the provided context to determine if the response is correct or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "sL4-XcjytWsu"
      },
      "outputs": [],
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[must_mention],\n",
        "    evaluators=[\n",
        "        \"criteria\",\n",
        "        \"cot_qa\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1RJr349zhv7"
      },
      "source": [
        "Task 3: Evaluating\n",
        "\n",
        "All that is left to do is evaluate our agent's response!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5TeCUUkuGld",
        "outputId": "cf2ee3b6-6853-41c6-8bb8-0b5f9741cdca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for project 'RAG Pipeline - Evaluation - aea38f41' at:\n",
            "https://smith.langchain.com/o/2f142be8-b860-5498-aaf0-55a67fe7e836/datasets/cebf9262-180b-4867-a341-bd77686964fa/compare?selectedSessions=6b9aea70-c94d-48df-8619-d9bf3679f4d0\n",
            "\n",
            "View all tests for Dataset Retrieval Augmented Generation - Evaluation Dataset - 5e646c3d at:\n",
            "https://smith.langchain.com/o/2f142be8-b860-5498-aaf0-55a67fe7e836/datasets/cebf9262-180b-4867-a341-bd77686964fa\n",
            "[------------------------------------------------->] 5/5"
          ]
        },
        {
          "data": {
            "text/html": [
              "<h3>Experiment Results:</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feedback.helpfulness</th>\n",
              "      <th>feedback.COT Contextual Accuracy</th>\n",
              "      <th>feedback.must_mention</th>\n",
              "      <th>error</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>run_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>b600b3f6-8bd7-439a-ad4e-fd0f0ac88187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.175753</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.335969</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.867373</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.929941</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.001336</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.532214</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.547899</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        feedback.helpfulness  feedback.COT Contextual Accuracy  \\\n",
              "count                    5.0                               5.0   \n",
              "unique                   NaN                               NaN   \n",
              "top                      NaN                               NaN   \n",
              "freq                     NaN                               NaN   \n",
              "mean                     1.0                               1.0   \n",
              "std                      0.0                               0.0   \n",
              "min                      1.0                               1.0   \n",
              "25%                      1.0                               1.0   \n",
              "50%                      1.0                               1.0   \n",
              "75%                      1.0                               1.0   \n",
              "max                      1.0                               1.0   \n",
              "\n",
              "       feedback.must_mention error  execution_time  \\\n",
              "count                      5     0        5.000000   \n",
              "unique                     1     0             NaN   \n",
              "top                     True   NaN             NaN   \n",
              "freq                       5   NaN             NaN   \n",
              "mean                     NaN   NaN        1.175753   \n",
              "std                      NaN   NaN        0.335969   \n",
              "min                      NaN   NaN        0.867373   \n",
              "25%                      NaN   NaN        0.929941   \n",
              "50%                      NaN   NaN        1.001336   \n",
              "75%                      NaN   NaN        1.532214   \n",
              "max                      NaN   NaN        1.547899   \n",
              "\n",
              "                                      run_id  \n",
              "count                                      5  \n",
              "unique                                     5  \n",
              "top     b600b3f6-8bd7-439a-ad4e-fd0f0ac88187  \n",
              "freq                                       1  \n",
              "mean                                     NaN  \n",
              "std                                      NaN  \n",
              "min                                      NaN  \n",
              "25%                                      NaN  \n",
              "50%                                      NaN  \n",
              "75%                                      NaN  \n",
              "max                                      NaN  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'project_name': 'RAG Pipeline - Evaluation - aea38f41',\n",
              " 'results': {'35abdb83-d9c4-4ba7-88ae-9a7a4c6b35d4': {'input': {'question': 'What is the chemical symbol for gold?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides the correct answer to the question asked in the input. The question asks for the chemical symbol for gold, and the submission correctly states that the chemical symbol for gold is Au. \\n\\nThe submission is helpful because it provides the information asked for in the input. It is insightful because it provides the correct scientific information. It is appropriate because it directly answers the question without adding unnecessary or irrelevant information. \\n\\nTherefore, the submission meets the criterion. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('d17eca97-9933-4e18-9e12-c319f2ae51c0'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer matches the context provided. The chemical symbol for gold is indeed Au. The student has provided the correct information.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('e5c96cac-5237-49aa-9635-a2e5b60d81a3'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 1.001336,\n",
              "   'run_id': 'b600b3f6-8bd7-439a-ad4e-fd0f0ac88187',\n",
              "   'output': 'The chemical symbol for gold is Au.',\n",
              "   'reference': {'must_mention': ['Au']}},\n",
              "  '8fbc3287-7a89-4c46-bf65-7aed94cb68d7': {'input': {'question': 'What is the largest planet in our solar system?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission is being evaluated on whether it is helpful, insightful, and appropriate. \\n\\nThe submission provides a direct and accurate answer to the question, which is \"What is the largest planet in our solar system?\" The answer given is \"The largest planet in our solar system is Jupiter.\" This is a correct and factual answer, making it helpful to anyone who needs this information.\\n\\nThe submission is insightful as it provides the exact information asked for in the input. It does not provide additional information or context, but the question does not ask for any, so this is not a problem.\\n\\nThe submission is appropriate as it directly addresses the question and provides the correct information in a clear and concise manner. It does not include any irrelevant or inappropriate content.\\n\\nTherefore, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('beaacb66-4996-4ff7-81c6-8578878a3627'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer matches the context provided. The context states that Jupiter is the largest planet in our solar system, and the student's answer also states that Jupiter is the largest planet in our solar system. Therefore, the student's answer is factually accurate.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('1af8ccbb-ebc3-45de-912d-65892c10c3e3'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 1.547899,\n",
              "   'run_id': '7a539be2-98f7-4d7f-9198-95738c0548b2',\n",
              "   'output': 'The largest planet in our solar system is Jupiter.',\n",
              "   'reference': {'must_mention': ['Jupiter']}},\n",
              "  '044f9d5b-813c-493f-a7a5-28eb8dfb2c02': {'input': {'question': 'Who painted the Mona Lisa?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission should be helpful, insightful, and appropriate. \\n\\nThe question asked is \"Who painted the Mona Lisa?\" \\n\\nThe submission provided the answer as \"The Mona Lisa was painted by Leonardo da Vinci.\" \\n\\nThis answer is helpful as it provides the exact information asked for in the question. \\n\\nThe answer is insightful as it provides the name of the artist who painted the Mona Lisa, which is a significant piece of information about the painting. \\n\\nThe answer is also appropriate as it directly addresses the question and does not include any irrelevant or inappropriate information. \\n\\nTherefore, the submission meets the criterion. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('236159f6-62f6-4a55-8ae2-5485638d84f2'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer matches the context provided. The context states that Leonardo da Vinci painted the Mona Lisa, and the student's answer also states that Leonardo da Vinci painted the Mona Lisa. Therefore, the student's answer is factually accurate.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('c9c88a1e-3330-4380-a3fd-c26e5e9c6906'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 0.929941,\n",
              "   'run_id': '08286812-69b4-49a9-a362-f8856acc1680',\n",
              "   'output': 'The Mona Lisa was painted by Leonardo da Vinci.',\n",
              "   'reference': {'must_mention': ['Leonardo da Vinci']}},\n",
              "  '85cef211-f44a-480d-bda7-97f538d882b5': {'input': {'question': 'What is the capital of France?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission is supposed to answer the question \"What is the capital of France?\" \\n\\nThe submitted answer is \"The capital of France is Paris.\" \\n\\nThis answer is helpful because it directly and accurately answers the question. \\n\\nThe answer is insightful because it provides the exact information that was asked for. \\n\\nThe answer is appropriate because it is relevant to the question and is presented in a respectful and professional manner. \\n\\nTherefore, the submission meets the criterion. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('4a6f6b7b-fb56-4929-a041-4189b6dfb87e'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer matches the context provided. The context states that the capital of France is Paris, and the student's answer also states that the capital of France is Paris. Therefore, the student's answer is factually accurate.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('0f68ff57-9bb5-443a-92a6-7821771280c3'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 0.867373,\n",
              "   'run_id': 'a1036370-a23f-405a-8c2c-850e77f736dd',\n",
              "   'output': 'The capital of France is Paris.',\n",
              "   'reference': {'must_mention': ['Paris']}},\n",
              "  '542db67e-f781-468e-af1c-4daabda8eea3': {'input': {'question': \"Who is the author of 'To Kill a Mockingbird'?\"},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment=\"The criterion for this task is helpfulness. \\n\\nThe submission is being evaluated on whether it is helpful, insightful, and appropriate. \\n\\nThe input asks for the author of 'To Kill a Mockingbird'. \\n\\nThe submission provides the correct answer, stating that Harper Lee is the author of 'To Kill a Mockingbird'. \\n\\nThis answer is helpful as it directly answers the question. \\n\\nThe answer is insightful as it provides the exact information asked for. \\n\\nThe answer is appropriate as it is relevant to the question and is presented in a respectful and professional manner. \\n\\nTherefore, the submission meets the criterion. \\n\\nY\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('ee4ecd34-e64a-47be-8149-0862f843db34'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer matches the context provided. The author of 'To Kill a Mockingbird' is indeed Harper Lee. The student has correctly identified the author.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('eea87be1-8a9f-46bb-b3ef-77845c227612'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 1.532214,\n",
              "   'run_id': '0222d2d4-899a-4714-8a51-d750983bc32c',\n",
              "   'output': 'The author of \"To Kill a Mockingbird\" is Harper Lee.',\n",
              "   'reference': {'must_mention': ['Harper Lee']}}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=agent_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=f\"RAG Pipeline - Evaluation - {uuid4().hex[0:8]}\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
